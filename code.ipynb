{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the game ready.\n",
    "\n",
    "from gym.envs import atari\n",
    "import torch  \n",
    "\n",
    "games = [\"adventure\",\n",
    "         \"air_raid\",\n",
    "         \"alien\",\n",
    "         \"crazy_climber\",\n",
    "         \"elevator_action\",\n",
    "         \"gravitar\",\n",
    "         \"keystone_kapers\",\n",
    "         \"king_kong\",\n",
    "         \"laser_gates\",\n",
    "         \"mr_do\",\n",
    "         \"ms_pacman\",\n",
    "         \"jamesbond\",\n",
    "         \"koolaid\",\n",
    "         \"zaxxon\"\n",
    "         # There are way more than this!\n",
    "         ]\n",
    "\n",
    "env = atari.AtariEnv(\n",
    "    game='pong',\n",
    "    frameskip=3,\n",
    "    obs_type='image'\n",
    ")\n",
    "env.reset()\n",
    "\n",
    "num_actions = len(env.get_action_meanings())\n",
    "\n",
    "# If some buttons are unused, ignore them. In Pong, I've decided to use just 3 moves.\n",
    "num_actions = 3\n",
    "\n",
    "def pick_move(move):\n",
    "    move = move.item()\n",
    "    if(num_actions == len(env.get_action_meanings())):\n",
    "       return(move)\n",
    "    move_dict = {\n",
    "        0 : 0,\n",
    "        1 : 2,\n",
    "        2 : 3\n",
    "    }\n",
    "    return(move_dict[move])\n",
    "\n",
    "print(env.get_action_meanings())\n",
    "\n",
    "for i in range(num_actions):\n",
    "    print(i, \", \", env.get_action_meanings()[pick_move(torch.tensor([i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to make images.\n",
    "            \n",
    "import matplotlib.pyplot as plt    \n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms.functional as F2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "        \n",
    "def image_to_ten(image):\n",
    "    image = torch.from_numpy(image).float()\n",
    "    image = image[34:-16,:,:] # I cut out the top and bottom, useless in Pong. \n",
    "    image = image.permute(-1,0,1)\n",
    "\n",
    "    image = F2.to_pil_image(image)\n",
    "    image = F2.to_grayscale(image)\n",
    "    image = F2.resize(image, 32)\n",
    "    image = F2.to_tensor(image)\n",
    "\n",
    "    image = image - torch.ones(image.shape) * image.min().item()\n",
    "    image = image / image.max().item()\n",
    "\n",
    "    theshold = .99\n",
    "    image[image < theshold] = torch.zeros(image[image < theshold].shape)\n",
    "    image[image > theshold] = torch.ones(image[image > theshold].shape)\n",
    "\n",
    "    image = image * 2 - torch.ones(image.shape)\n",
    "\n",
    "    return(image.to(device))\n",
    "\n",
    "def show_image(image):\n",
    "    image = (image + torch.ones(image.shape).to(device)) / 2\n",
    "    image = image.permute(1,2,0).cpu()\n",
    "    if(image.shape[-1] == 1):\n",
    "        image = torch.cat([image, image, image],dim=-1)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.ioff()\n",
    "    \n",
    "    \n",
    "    \n",
    "### Play at random for a bit, just to show an interesting image.\n",
    "\n",
    "import random\n",
    "\n",
    "def move_random():\n",
    "    env.step(pick_move(torch.tensor([random.randrange(num_actions)])))\n",
    "    \n",
    "for i in range(50):\n",
    "    move_random()\n",
    "    \n",
    "    \n",
    "\n",
    "image_1 = env._get_image()\n",
    "image_1 = image_to_ten(image_1)\n",
    "show_image(image_1)\n",
    "\n",
    "two_images = torch.cat([image_1, image_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Policy converting observations into action-probabilities. \n",
    "\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "    \n",
    "class ConstrainedConv2d(nn.Conv2d):\n",
    "    def forward(self, input):\n",
    "        return nn.functional.conv2d(input, self.weight.clamp(min=-1.0, max=1.0), self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        example = torch.zeros(two_images.shape).unsqueeze(0)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            ConstrainedConv2d(\n",
    "                in_channels = 2, \n",
    "                out_channels = 4,\n",
    "                kernel_size = (3,3),\n",
    "                stride = (2,2),\n",
    "                padding = (1,1)\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            ConstrainedConv2d(\n",
    "                in_channels = 4, \n",
    "                out_channels = 4,\n",
    "                kernel_size = (3,3),\n",
    "                stride = (2,2),\n",
    "                padding = (1,1)\n",
    "            ),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        example = self.cnn(example)\n",
    "        quantity = np.product(example.shape)\n",
    "  \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features = quantity,\n",
    "                out_features = num_actions),\n",
    "            nn.Softmax(dim = -1))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if(len(x.shape) == 3):\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        x = self.lin(x)\n",
    "        return(x)\n",
    "        \n",
    "    def get_move(self, move):\n",
    "        moves = [i for i in range(num_actions)]\n",
    "        move = move.squeeze(0).tolist()\n",
    "        move = torch.tensor(random.choices(moves, weights = move)).to(device).squeeze(0)\n",
    "        return(move)\n",
    "    \n",
    "\n",
    "        \n",
    "dqn = DQN().to(device)\n",
    "opti = optim.Adam(dqn.parameters())\n",
    "\n",
    "# If you've already started training, load the saved model\n",
    "#import os\n",
    "#os.chdir(\"C://Users//tedjt//Desktop//Thinkster//77 More Pong//code\")\n",
    "#dqn.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "print(dqn)\n",
    "print()\n",
    "\n",
    "summary(dqn, two_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to manage rewards\n",
    "\n",
    "GAMMA = 0.99\n",
    "            \n",
    "def discount_rewards(r):\n",
    "    discounted_r = torch.zeros(r.shape).to(device)\n",
    "    length = list(r.shape)[0]\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, length)):\n",
    "        if r[t].item() != 0: \n",
    "            running_add = 0 \n",
    "        running_add = running_add * GAMMA + r[t].item() \n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "# For example:\n",
    "rewards = torch.zeros((24,))\n",
    "rewards[len(rewards)//3 - 1] = 1\n",
    "rewards[-1] = -1\n",
    "print(rewards)\n",
    "print(discount_rewards(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to remember stuff\n",
    "            \n",
    "class memory():\n",
    "    def __init__(self):\n",
    "        self.mem = []\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.mem.append(*args)\n",
    "        \n",
    "    def empty(self):\n",
    "        self.mem = []\n",
    "        \n",
    "mem = memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update the policy based on memory\n",
    "\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "def duration():\n",
    "    change_time = datetime.datetime.now() - start_time\n",
    "    change_time = change_time - datetime.timedelta(microseconds=change_time.microseconds)\n",
    "    return(change_time)\n",
    "\n",
    "def update():\n",
    "    batch = mem.mem\n",
    "    mem.empty()\n",
    "        \n",
    "    moves_batch = torch.cat([b[0] for b in batch], dim = 0)\n",
    "    \n",
    "    move_batch = torch.cat([b[1].unsqueeze(0) for b in batch], dim = 0)\n",
    "    move_batch = F.one_hot(move_batch, num_classes = num_actions).float()\n",
    "    \n",
    "    reward_batch = torch.cat([b[2].unsqueeze(0) for b in batch], dim = 0)\n",
    "    total_reward = int(reward_batch.sum().item())\n",
    "    reward_batch = discount_rewards(reward_batch)\n",
    "    reward_batch = torch.cat([reward_batch.unsqueeze(-1) for i in range(num_actions)], dim = -1)\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Reward:\", total_reward)\n",
    "    print(\"Min/Max:\", round(moves_batch.min().item(),3), round(moves_batch.max().item(),3))\n",
    "    print(\"Time:\", duration())\n",
    "\n",
    "    criterion = nn.BCELoss(weight = reward_batch)\n",
    "    loss = criterion(moves_batch, move_batch)\n",
    "    opti.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in dqn.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    opti.step()\n",
    "    \n",
    "    return(total_reward, moves_batch.min().item(), moves_batch.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to train!\n",
    "    \n",
    "def train():\n",
    "    last_state = None\n",
    "    state = image_to_ten(env.reset())\n",
    "    # In games besides Pong, negative-rewarding life-loss might help\n",
    "    #old_lives = 5\n",
    "    while(True):\n",
    "        env.render()\n",
    "        #show_image(state)\n",
    "        observation = torch.cat([state, last_state]) if last_state != None else torch.cat([state, state])\n",
    "        moves = dqn(observation)\n",
    "        move = dqn.get_move(moves)\n",
    "        next_state, reward, done, lives = env.step(pick_move(move))\n",
    "        #lives = lives[\"ale.lives\"]\n",
    "        #if(lives < old_lives):\n",
    "        #    reward -= 1\n",
    "        #    old_lives = lives\n",
    "        next_state = image_to_ten(next_state)\n",
    "        mem.push((moves, move, torch.tensor(reward).to(device)))\n",
    "        last_state = state\n",
    "        state = next_state\n",
    "        if(done):\n",
    "            break\n",
    "    torch.save(dqn.state_dict(), \"model.pt\")\n",
    "    return(update())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot training.\n",
    "    \n",
    "plot_len = 100\n",
    "\n",
    "def plot_it(rewards, mins, maxes):\n",
    "    xs = [i for i in range(len(rewards))]\n",
    "    \n",
    "    if(xs[-1] > plot_len):\n",
    "        xs = xs[-plot_len:]\n",
    "        rewards = rewards[-plot_len:]\n",
    "        mins = mins[-plot_len:]\n",
    "        maxes = maxes[-plot_len:]\n",
    "    \n",
    "    plt.plot(xs, rewards)\n",
    "    plt.plot(xs, [0 for i in range(len(xs))])\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.ioff()\n",
    "    \n",
    "    plt.plot(xs, mins)\n",
    "    plt.plot(xs, maxes)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train!\n",
    "    \n",
    "rewards = []\n",
    "mins = []\n",
    "maxes = []\n",
    "i = 0\n",
    "\n",
    "while(True):\n",
    "    i += 1\n",
    "    reward, m_1, m_2 = train()\n",
    "    print(\"Games:\", i)\n",
    "    rewards.append(reward)\n",
    "    mins.append(m_1)\n",
    "    maxes.append(m_2)\n",
    "    plot_it(rewards, mins, maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
